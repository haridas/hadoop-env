Hadoop cluster setup
====================
:toc2:
:numbered:

== Hadoop Cluster

Setup standard hadoop in cluster mode on single node or multiple node environment.
Hadoop cluster mode means, the individual components are run separately on single machine
as separate JVMs or running on multiple machines. There is an single machine setup
version of hadoop to quickly play around with its features, which make use only one
JVM to handle all the components.

Here we are trying to setup a hadoop in cluster mode, one single machine with multiple
jvms or multiple machines. We are using docker image to package and run the hadoop tools.

Main Hadoop services are:-

1. namenode ( storage )
2. datanode ( storage )
3. resource manager ( computation )
4. node manager ( computation )

One hadoop cluster can formed by one namenode and multiple datanodes.
The resource manager runs on same machine as namenode ( simpler setup). Responsibility of namenode is store metadata about distributed
filesystem (hdfs). Datanode actually stores the data in blob form,
and nodemanager will be running on each datanode to handle actual
computation requests from resource manager.

== Prerequisites
As all cluster environments, network address resolution for each node is a key requirement
for stable setup. Ideally a local DNS setup which permanently allocate a hostnames
to all the nodes in the network. Or we can manually set the hostnames without DNS.


=== Get hadoop docker image

### From docker hub
```bash
docker pull haridasn/hadoop-2.8.5:latest
```
### Build the docker image locally (Optional)

```bash
git clone https://github.com/haridas/hadoop-env
cd hadoop-env/docker
docker build -t hadoop-2.8.5:latest
```

=== Set correct hostnames for multi-node cluster setup
NOTE: If you setting the cluster on same machine with docker, you can skip this step.

Each node in the host machine can reach each other using the hostname.

Hadoop cluster setup using multiple physical machines.

1. set hostnames correctly.

All the nodes place the same set of values.

```bash
cat /etc/hosts
master  <ip-address>
node1
node2
node3
..
..
```

2. Set the hostname of the machine to match this address.

edit `/etc/hostname`
edit `/etc/hosts` and replace any occurrence of old hostname with new one.

3. Update and check hostname changed correctly

```bash
sudo hostname <hostname>
hostname
```
Cross check all the machines have correct set of hostnames before going to next
step.

== Setup cluster on single machine using docker

We are using the docker container mainly to isolate the processes, for a simpler
setup on single machine we make use of the same network stack as the host machine.

=== Start namenode and resource Manager

The option `--net host` bellow allow us to use the host machine's network stack.

```
docker run -it -d --name namenode --net host haridasn/hadoop-2.8.5:latest namenode <host-ip>

# check container is running
docker ps -a

# Check container logs
docker logs -f namenode

```

Optionally you can run the Namenode and Resource manager without
sharing the host-network. In this case docker abstract the network
stack by itself, so no interference with host network and ports.

There is one advantage with this method, we can run multiple
namenode(s)/datanode(s) on same machine without `port` collision.

=== Start datanode and resource manager

```bash
docker run -it -d --name datanode1 --net host haridasn/hadoop-2.8.5:latest datanode <name-node-ip>

docker ps -a

docker logs -f datanode1

```

IMPORTANT: Since we are sharing the host network stack, we can't run
more than one datanode single machine setup.


### Get the client tools setup on your host
```bash

# copy a fresh copy of binaries into host machine, now you are ready to connect
# your hadoop cluster using the client tools.
docker cp datanode:/opt/hadoop .
export PATH=`pwd`/hadoop/bin:$PATH

```

### Check hdfs
```bash
hdfs dfs -ls /
sudo -E ./hadoop/bin/hdfs dfs -put /var/log/supervisor /logs
sudo -E ./hadoop/bin/hdfs dfs -put /etc/passwd /passwd
sudo -E ./hadoop/bin/hdfs dfs -cp /passwd /passwdr

```

### Check Resource manager works fine

```bash
yarn jar `pwd`/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar
pi 1 1

yarn jar `pwd`/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar
wordcount /logs/* /out/
```

## Other Bigdata tools on hadoop environment

## load pig

1. Download and extract it

```
wget http://mirrors.estointernet.in/apache/pig/pig-0.17.0/pig-0.17.0.tar.gz
```


2. Setup pig and configure it with hadoop cluster.


```bash
export PIG_HOME=<path-to-pig-home>
export PATH=$PATH:$PIG_HOME/bin
export PIG_CLASSPATH=<path-to-hadoop-conf-dir>

pig
```

3. Load test data for testing

```bash
sudo -E ./hadoop/bin/hdfs dfs -mkdir /pig
sudo -E ./hadoop/bin/hdfs dfs -put pig/tutorial/data /pig/data
```


4. Try pig commands

```bash
pig
# tutorials folder have set of commands try out each and see how it works.

```

Pig commands

```
raw = LOAD '/pig/data/excite-small.log' USING PigStorage('\t') AS (user, time,
query);
user = filter raw by $2=='powwow.com';

dump user

```


## Test hive

SQL interface over hadoop system.

http://mirrors.estointernet.in/apache/hive/hive-3.1.1/apache-hive-3.1.1-bin.tar.gz
